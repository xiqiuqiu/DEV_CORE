[
  {
    "title": "什么是乐观更新（Optimistic UI）",
    "author": "MR.Z",
    "date": "2026-01-19T00:00:00.000Z",
    "tags": [
      "React",
      "前端性能",
      "乐观更新",
      "用户体验",
      "LocalStorage"
    ],
    "summary": "拒绝转圈等待！深入探讨前端乐观更新（Optimistic UI）技术，通过\"先斩后奏\"的策略实现极致响应。本文详解三步工作流与React实现，并诚实分析了跨设备同步等\"坑点\"。",
    "slug": "what-is-optimistic-ui",
    "fileName": "什么是乐观更新（Optimistic UI）.md",
    "content": "\r\n# 拒绝转圈等待：用乐观更新（Optimistic UI）骗过用户的眼睛\r\n\r\n## 那个该死的 Loading\r\n\r\n点击发送，转圈，两秒后消息上屏。这流程太熟悉了，也太迟钝了。用户其实不在乎你的 API 到底处理了多久，他们只觉得你的应用卡。\r\n\r\n有没有办法不改后端还能让体验丝般顺滑？有，\"乐观更新\" (Optimistic UI)。说白了就是：**先假装成功，再校对结果。**\r\n\r\n## 它是怎么骗人的？\r\n\r\n用户点发送，前端直接把消息塞进列表，告诉用户\"发好了\"。然后悄悄在后台请求 API。\r\n\r\n- **成功了**？什么都不用做，或者默默更新一下真实 ID。\r\n- **失败了**？再告诉用户\"发送失败\"，或者回滚状态。\r\n\r\n这种\"先斩后奏\"的策略，能在这个网络延迟不可避免的世界里，通过欺骗视觉感官，制造出\"零延迟\"的假象。\r\n\r\n## 怎么做？（不用后端配合版）\r\n\r\n如果是纯内存操作，刷新一下页面，那个正在\"假装发送\"的消息就丢了。所以我们要用到 `LocalStorage` 来兜底。\r\n\r\n### 核心三步走\r\n\r\n1.  **发送时：双写**\r\n    生成一个临时 ID，把消息同时写入 `LocalStorage` 和 UI 列表。然后才去调 API。\r\n\r\n2.  **渲染时：合并**\r\n    列表数据 = 后端返回的历史记录 + `LocalStorage` 里的待发送记录。\r\n    _别忘了去重：如果后端已经返回了这条消息，就不要显示本地那条假的了。_\r\n\r\n3.  **结果回来后：清理**\r\n    - **成功**：删掉 `LocalStorage` 里的临时数据。\r\n    - **失败**：在 `LocalStorage` 里标记状态为 error，UI 上给个重试按钮。\r\n\r\n## React 代码大概长这样\r\n\r\n别整复杂的，看核心逻辑：\r\n\r\n```javascript\r\nconst CONVERSATION_KEY = `chat_pending_${conversationId}`;\r\n\r\n// 发送逻辑\r\nconst handleSend = async (text) => {\r\n  const tempMsg = {\r\n    id: Date.now(),\r\n    role: \"user\",\r\n    content: text,\r\n    status: \"sending\",\r\n  };\r\n\r\n  // 1. 先骗用户：存本地，更 UI\r\n  localStorage.setItem(CONVERSATION_KEY, JSON.stringify(tempMsg));\r\n  setMessages((prev) => [...prev, tempMsg]);\r\n\r\n  try {\r\n    // 2. 再干正事\r\n    await api.sendMessage(text);\r\n\r\n    // 3. 成功了解除伪装\r\n    localStorage.removeItem(CONVERSATION_KEY);\r\n    fetchHistory(); // 重新拉取以确保数据一致\r\n  } catch (error) {\r\n    // 4. 演砸了：标记失败\r\n    tempMsg.status = \"error\";\r\n    localStorage.setItem(CONVERSATION_KEY, JSON.stringify(tempMsg));\r\n    forceUpdate(); // 触发 UI 更新显示失败状态\r\n  }\r\n};\r\n\r\n// 加载逻辑\r\nuseEffect(() => {\r\n  api.getHistory().then((serverList) => {\r\n    // 看看本地有没有刚才没发完的\r\n    const cachedMsg = JSON.parse(localStorage.getItem(CONVERSATION_KEY));\r\n\r\n    if (cachedMsg && !serverList.some((m) => m.content === cachedMsg.content)) {\r\n      setMessages([...serverList, cachedMsg]);\r\n    } else {\r\n      setMessages(serverList);\r\n    }\r\n  });\r\n}, []);\r\n```\r\n\r\n## 真的完美吗？并不是\r\n\r\n这招有个致命伤：**跨设备不同步**。\r\n\r\n你在电脑上发的消息，因为是存在电脑浏览器的 LocalStorage 里，在你手机上是看不见的。直到后端真正处理完并落库。\r\n如果你的用户频繁在设备间切换（比如发完消息立刻拿起手机看），这可能会是个问题。但在大多数场景下，这个短暂的延迟是可以接受的。\r\n\r\n还有就是**代码复杂度的增加**。你要处理去重、合并、请求失败后的重试、极端情况下的缓存清理... 维护成本比简单的\"转圈等待\"高了不少。\r\n\r\n## 总结\r\n\r\n乐观更新是典型的\"脏活累活前端干\"。\r\n\r\n如果后端能配合改造成 WebSocket 或者强大的同步机制，那最好。但如果你只能动前端，又想让应用快得像本地 App，这是性价比最高的方案。用不用，取决于你有多得罪不起你的用户体验。\r\n"
  },
  {
    "title": "Claude Skills 的 5 个“反常识”设计",
    "author": "MR.Z",
    "date": "2026-01-19T00:00:00.000Z",
    "tags": [
      "Claude",
      "AI",
      "Skills",
      "Anthropic",
      "提示工程"
    ],
    "summary": "Claude Skills 并不像传统的函数调用。它更像是一套精密的提示词模板，没有任何代码层面的路由算法，全靠大模型\"硬扛\"。本文解析它五个反直觉的设计细节。",
    "slug": "claude-skills-highlights",
    "fileName": "揭秘 Claude Skills：你不知道的 5 个“反常识”亮点.md",
    "content": "\r\n# Claude Skills 的 5 个“反常识”设计\r\n\r\n通常我们给 AI 加“技能”，想到的都是写代码：定义函数、配置 API、通过 Function Calling 挂载。这很符合程序员的直觉。\r\n\r\n但 Anthropic 的 Claude Skills 走了一条完全不同的路。它没有试图把 LLM 塞进代码的框架里，而是反过来，让工具去适应 LLM 的语言本能。\r\n\r\n仔细研究一下它的反直觉设计，你会发现这不仅是技术选型的问题，更像是对 AI 未来交互形态的一种赌注。\r\n\r\n## 1. 技能不是代码，是“提示词包”\r\n\r\n大多数开发者习惯把“技能”等同于“可执行代码”。但在 Claude Skills 里，技能通过 `prompt expansion`（提示词扩展）和 `context modification`（上下文修改）来运作。\r\n\r\n简而言之，它不直接“跑”任务。它是把一段写好的、针对特定领域的知识和指令，塞进对话的上下文里，然后让 Claude 自己去推理。\r\n\r\nHan Lee 的总结很到位：\r\n\r\n> Skills are not executable code... Skills are specialized prompt templates that inject domain-specific instructions into the conversation context.\r\n\r\n这其实把“各司其职”的界限打破了。开发者不再是写僵化的接口，而是在写一份份 AI 能读懂的“说明书”。Simon Willison 觉得这种设计“简单得惊人”：一个 Markdown 文件告诉模型怎么做，仅此而已。\r\n\r\n## 2. 没有路由算法，全靠 LLM 硬扛\r\n\r\n在复杂的 AI 系统里，如何把用户的 query 路由到正确的工具通常是个算法问题：用向量搜索、关键词分类器，或者专门的意图识别模块。\r\n\r\nClaude Skills 没有任何这种代码层面的辅助。\r\n\r\n它的选择机制非常“暴力”：它把所有可用技能的清单（名字和简介）直接扔给 Claude，然后让模型自己读、自己选。\r\n\r\nClaude 面向的其实只有一个通用的“元工具” `Skill`。当它觉得用户需要处理 PDF 时，它就调用 `Skill(command='pdf')`。\r\n\r\n这种设计非常依赖模型本身的推理能力。Anthropic 显然在赌：随着模型变强，LLM 本身的模糊判断力，会比人类写死的路由逻辑更靠谱。\r\n\r\n## 3. 简单到像个草台班子\r\n\r\n第一眼看到 Claude Skills 的实现，你可能会觉得简陋：就这？一个 Markdown 文件配几个脚本？\r\n\r\n但这种“简陋”恰恰是它的杀手锏。相比于 Model Context Protocol (MCP) 那种虽然强大但复杂的协议规范，Skills 的门槛极低。只要你会写文档，你就能写 Skill。\r\n\r\n这种低门槛意味着极高的可扩展性。不需要繁重的 SDK，不需要复杂的鉴权配置，就是一个文档的事儿。这让社区快速分享和复用技能成为了可能。\r\n\r\n## 4. 给 AI 留了套“暗号”\r\n\r\n为了不让冗长的技能说明书干扰用户的阅读，Claude Skills 设计了一套双通道机制，利用 `isMeta` 标志来区分受众。\r\n\r\n- **给用户看 (isMeta: false)**：简短的状态更新，比如 `<command-message>正在加载 PDF 处理技能...</command-message>`。\r\n- **给 AI 看 (isMeta: true)**：技能文件 `SKILL.md` 的全部内容。这部分直接注入上下文，但对用户隐藏。\r\n\r\n这种分离很聪明。它既保证了 AI 拿到了它需要的详尽指令，又没让用户的屏幕被几千字的 Prompt 刷屏。这不仅仅是 UI 优化，更是一种针对 AI 交互特有的信息架构。\r\n\r\n## 5. 为了省 Token，它真的很“抠门”\r\n\r\n如果一上来就把几百个技能的说明书全塞进 Context，多少 Token 也不够烧的。\r\n\r\nClaude Skills 采用了一种极其保守的“渐进式披露”（Progressive Disclosure）策略：\r\n\r\n1.  **初始阶段**：只加载所有技能的名字和一句话简介。Han Lee 提到这甚至有个 15000 字符的硬限制，逼着你写简介尽量精简。\r\n2.  **调用阶段**：只有当 Claude 决定用某个技能了，系统才会去加载那个技能对应的 `SKILL.md`。\r\n3.  **执行阶段**：就算加载了 `SKILL.md`，里面引用的外部脚本或文档，也只在真正用到时才会通过 Read 工具按需读取。\r\n\r\n这种层层递进的加载方式，是整个系统能 scale 的关键。否则，技能稍微多几个，上下文窗口就爆了。\r\n\r\n---\r\n\r\nClaude Skills 给人的感觉是“重语言，轻代码”。它不再试图把 AI 变成一个精准执行的机器，而是把指令、知识和工作流都自然语言化，构建了一个更松散、但也许更具生命力的系统。\r\n"
  }
]